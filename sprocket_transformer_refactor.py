"""SPROCKET transformer."""

import numpy as np
from collections import Counter
from supplemental_metrics import compute_signed_emd, sorted_emd_wrapper, cosine_similarity_wrapper, correlation_wrapper, chebyshev_wrapper
import os
from numba import get_num_threads, njit, prange, set_num_threads

from aeon.transformations.collection import BaseCollectionTransformer, Normalizer
from aeon.utils.validation import check_n_jobs

from aeon.distances import msm_distance, euclidean_distance, adtw_distance, dtw_distance, erp_distance, lcss_distance, twe_distance, wdtw_distance, manhattan_distance, squared_distance

MSM = 0
EUCLIDEAN = 1
ADTW = 2
DTW = 3
WDTW = 4
ERP = 5
LCSS = 6
TWE = 7
SQUARED = 8
MANHATTAN = 9
EMD_UNSORT = 10
EMD_SORT = 11
COSINE = 12
CHEBYSHEV = 13
CORRELATION = 14


class SPROCKETTransformer(BaseCollectionTransformer):

    # Mapping for strings (case-insensitive) to constants
    DIST_MAP = {
        'msm': MSM,
        'euclidean': EUCLIDEAN,
        'adtw': ADTW,
        'dtw': DTW,
        'wdtw': WDTW,
        'erp': ERP,
        'lcss': LCSS,
        'twe': TWE,
        'squared': SQUARED,
        'emd_unsort': EMD_UNSORT,
        'emd_sort': EMD_SORT,
        'manhattan': MANHATTAN,
        'cosine': COSINE,
        'chebyshev': CHEBYSHEV,
        'correlation': CORRELATION
    }

    """Selected Prototype RandOm Convolutional KErnel Transform: 
    Combining ROCKET and Prototype Learning.
    
    RandOm Convolutional KErnel Transform (ROCKET).

    A kernel (or convolution) is a subseries used to create features that can be used
    in machine learning tasks. ROCKET [1]_ generates a large number of random
    convolutional kernels in the fit method. The length and dilation of each kernel
    are also randomly generated. A kernel is used to create an activation map for
    each series by running it across a time series, including random length and
    dilation. It transforms the time series and is used as a randomization method
    to produce a transformed set of datapoints.
    
    Selected Prototype
    
    After convolving a series, new features are generated by comparing the series
    to a number of reference prototypes. These points are chosen randomly,
    through stratified sampling, or via clustering (under development).
    Prototypes are compared with a distance measure and the resulting distances
    are the pooling features for ROCKET. Supported distances are in DIST_MAP. 
    Only independent distance calculations for multivariate series are available, 
    since ROCKET may zero some channels.
    
    *LCSS is available, but the ROCKET convolutional kernels sometimes cause divide 
    by zero errors if LCSS is used without turning.
    
    #Preliminary Results
    
    From small scale testing, random convolved point distances are good pooling features  
    for ROCKET, but require substantially more computation. Accuracy is better than NN, 
    but worse than MiniRocket, with less runtime PF, but more than ROCKET if used
    with elastic measures.
    
    #Currently in Alpha, plan to improve and remain compatible with Aeon toolkit.
    
    TODO
    
    Prototypes chosen via clustering (will increase runtime)
    MiniRocket Kernels
    Improve Convolution Function (will decrease runtime)
    Make pairwise distance numba compatible (will require aeon support)
    Code Cleanup

    Parameters
    ----------
    n_kernels : int, default=512
       Number of random convolutional kernels.
    normalise : bool, default True
       Whether or not to normalise the input time series per instance.
    n_jobs : int, default=1
       The number of jobs to run in parallel for `transform`. ``-1`` means using all
       processors.
    random_state : None or int, optional, default = None
        Seed for random number generation.
    proto_per_kernel : float, default=4.0
        Default chooses ceil(log_proto_per_kernel(|X|)), where X is the test dataset.
    window_frac: float, default = .2
        Bounding matrix window size for elastic distance measures
    stratified_sampling : bool, default False
        whether to use stratified random sampling to select prototypes
        
        
    See Also
    --------
    MiniRocket, MultiRocket, HYDRA

    References
    ----------
    .. [1] Tan, Chang Wei and Dempster, Angus and Bergmeir, Christoph
        and Webb, Geoffrey I,
        "ROCKET: Exceptionally fast and accurate time series
      classification using random convolutional kernels",2020,
      https://link.springer.com/article/10.1007/s10618-020-00701-z,
      https://arxiv.org/abs/1910.1305
     
    Example
    --------
    >>> from sprocket_transformer import SPROCKETTransformer
    >>> from aeon.datasets import load_unit_test
    >>> X_train, y_train = load_unit_test(split="train")
    >>> X_test, y_test = load_unit_test(split="test")
    >>> trf = SPROCKETTransformer(n_kernels=512)
    >>> trf.fit(X_train)
    >>> X_train = trf.transform(X_train)
    >>> X_test = trf.transform(X_test)
    """

    _tags = {
        "output_data_type": "Tabular",
        "capability:multivariate": True,
        "capability:multithreading": True,
        "algorithm_type": "convolution",
        "X_inner_type": "numpy3D",
    }

    def __init__(
        self,
        n_kernels=512,
        normalise=False,
        n_jobs=1,
        random_state=1,
        proto_per_kernel=4.0,
        window_frac=.2,
        dist_id='euclidean',
        stratified_sampling=False
    ):
        self.n_kernels = n_kernels
        self.normalise = normalise
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.proto_per_kernel = proto_per_kernel
        self.window_frac = window_frac
        # Set dist_id from string or int
        if isinstance(dist_id, int):
            self.dist_id = dist_id
        elif isinstance(dist_id, str):
            self.dist_id = self.DIST_MAP[dist_id.lower()]
        else:
            raise ValueError(f"Invalid dist_name type: {type(dist_name)}") 
        self.stratified_sampling = stratified_sampling
        super().__init__()
        

    def _fit(self, X, y=None):
        """Generate random kernels adjusted to time series shape.

        Infers time series length and number of channels from input numpy array,
        and generates random kernels and prototypes.

        Parameters
        ----------
        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)
            collection of time series to transform
        y : ignored argument for interface compatibility

        Returns
        -------
        self
        """
        self._n_jobs = check_n_jobs(self.n_jobs)

        if isinstance(self.random_state, int):
            self._random_state = self.random_state
        else:
            self._random_state = None
        n_channels = X[0].shape[0]

        # The only use of n_timepoints is to set the maximum dilation
        self.fit_min_length_ = X[0].shape[1]
        self.kernels = _generate_kernels(
            self.fit_min_length_, self.n_kernels, n_channels, self._random_state
        )
        #print('kernels_time', after_kernels_time-before_kernels_time)
        '''for i in range(0, len(self.kernels)):
            print(i, self.kernels[i])'''
        #exit()
        self.kernel_points = _get_prototype_points(np.array(X, dtype=np.float32), y, self.kernels, self.proto_per_kernel, self.window_frac, self.stratified_sampling)
        #print('prototype_time', all_end-all_start)
        return self

    def _transform(self, X, y=None):
        """Transform input time series using random convolutional kernels.

        Parameters
        ----------
        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)
            collection of time series to transform
        y : ignored argument for interface compatibility

        Returns
        -------
        np.ndarray (n_cases, n_kernels), transformed features
        """
        if self.normalise:
            norm = Normalizer()
            X = norm.fit_transform(X)

        prev_threads = get_num_threads()
        set_num_threads(self._n_jobs)
        X = np.asarray(X)
        X_ = apply_kernels_proto(X, y, self.kernels, self.kernel_points, self.dist_id, self.window_frac)
        set_num_threads(prev_threads)
        return X_

@njit(
    fastmath=True,
    cache=True,
)
def stratified_random_indices_numba(labels, k):
    """
    Stratified random selection of exactly k sample indices.

    Guarantees:
      • Approximate class balance (by frequency)
      • Exactly k total samples
      • If k >= number of classes → every class has ≥1 sample
      • If k < number of classes → randomly choose which classes to include
      • Works for any integer or non-contiguous label values
    """

    n = labels.shape[0]

    unique_labels = np.unique(labels)
    n_classes = unique_labels.shape[0]

    remapped_labels = np.empty(n, dtype=np.int64)
    for i in range(n):
        # find the class index for this label
        label = labels[i]
        idx = 0
        for j in range(n_classes):
            if unique_labels[j] == label:
                idx = j
                break
        remapped_labels[i] = idx

    counts = np.zeros(n_classes, dtype=np.int64)
    for i in range(n):
        counts[remapped_labels[i]] += 1

    # --- Choose which classes to include ---
    if k < n_classes:
        cls_arr = np.arange(n_classes)
        np.random.shuffle(cls_arr)
        chosen_classes = cls_arr[:k]
        n_sel_classes = k
    else:
        chosen_classes = np.arange(n_classes)
        n_sel_classes = n_classes

    total_counts = 0
    for i in range(n_sel_classes):
        total_counts += counts[chosen_classes[i]]

    per_class = np.zeros(n_sel_classes, dtype=np.int64)
    for i in range(n_sel_classes):
        c = chosen_classes[i]
        per_class[i] = (counts[c] * k) // total_counts

    if k >= n_sel_classes:
        for i in range(n_sel_classes):
            if per_class[i] == 0:
                per_class[i] = 1

    total_alloc = 0
    for i in range(n_sel_classes):
        total_alloc += per_class[i]
    diff = k - total_alloc

    tmp_order = np.arange(n_sel_classes)
    np.random.shuffle(tmp_order)
    if diff > 0:
        for i in range(diff):
            per_class[tmp_order[i % n_sel_classes]] += 1
    elif diff < 0:
        diff = -diff
        for i in range(diff):
            j = tmp_order[i % n_sel_classes]
            if per_class[j] > 1:
                per_class[j] -= 1

    result = np.empty(k, dtype=np.int64)
    pos = 0
    for i in range(n_sel_classes):
        cls = chosen_classes[i]
        n_pick = per_class[i]
        if n_pick == 0:
            continue

        cnt = counts[cls]
        cls_indices = np.empty(cnt, dtype=np.int64)
        t = 0
        for j in range(n):
            if remapped_labels[j] == cls:
                cls_indices[t] = j
                t += 1

        np.random.shuffle(cls_indices)
        for j in range(n_pick):
            result[pos] = cls_indices[j]
            pos += 1

    np.random.shuffle(result)
    return result

@njit(fastmath=True, cache=True)
def _generate_kernels(n_timepoints, n_kernels, n_channels, seed):
    if seed is not None:
        np.random.seed(seed)
    candidate_lengths = np.array((7, 9, 11), dtype=np.int32)
    lengths = np.random.choice(candidate_lengths, n_kernels).astype(np.int32)
    
    num_channel_indices = np.zeros(n_kernels, dtype=np.int32)
    for i in range(n_kernels):
        limit = min(n_channels, lengths[i])
        num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))

    channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)

    weights = np.zeros(
        np.int32(
            np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))
        ),
        dtype=np.float32,
    )
    biases = np.zeros(n_kernels, dtype=np.float32)
    dilations = np.zeros(n_kernels, dtype=np.int32)
    paddings = np.zeros(n_kernels, dtype=np.int32)
    offsets = np.zeros(
            sum(lengths), dtype=np.int32,
    )
    a1 = 0  # for weights
    a2 = 0  # for channel_indices
    for i in range(n_kernels):
        _length = lengths[i]
        _num_channel_indices = num_channel_indices[i]
        
        
        
        _weights = np.random.normal(0, 1, _num_channel_indices * _length).astype(
            np.float32
        )

        b1 = a1 + (_num_channel_indices * _length)
        b2 = a2 + _num_channel_indices

        a3 = 0  # for weights (per channel)
        for _ in range(_num_channel_indices):
            b3 = a3 + _length
            _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()
            a3 = b3
        weights[a1:b1] = _weights

        channel_indices[a2:b2] = np.random.choice(
            np.arange(0, n_channels), _num_channel_indices, replace=False
        )

        biases[i] = np.random.uniform(-1, 1)

        dilation = 2 ** np.random.uniform(
            0, np.log2((n_timepoints - 1) / (_length - 1))
        )
        dilation = np.int32(dilation)
        dilations[i] = dilation

        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0
        paddings[i] = padding

        a1 = b1
        a2 = b2

    return (
        weights,
        lengths,
        biases,
        dilations,
        paddings,
        num_channel_indices,
        channel_indices,
    )


#Faster without numba on UCR/UEA
'''@njit(
    parallel=True,
    fastmath=True,
    cache=True,
)'''
def _get_prototype_points(X, y, kernels, proto_per_kernel=4.0, window_frac=.2, stratified_random_sampling=True):
    """
    Select prototypical points per kernel by convolving sampled inputs.
    """
    weights, lengths, biases, dilations, paddings, n_channel_indices, channel_indices = kernels
    
    #n_cases = X.shape[0]
    n_cases = 1 + int(np.log(X.shape[0]) / np.log(proto_per_kernel))
    
    n_channels, n_timepoints = X[0].shape
    n_kernels = len(lengths)
    
    # Compute start and end indices for weights and channels
    weight_starts = np.zeros(n_kernels, dtype=np.int32)
    weight_ends = np.zeros(n_kernels, dtype=np.int32)
    channel_starts = np.zeros(n_kernels, dtype=np.int32)
    channel_ends = np.zeros(n_kernels, dtype=np.int32)
    
    w_idx = 0
    c_idx = 0

    for i in prange(n_kernels):
        weight_starts[i] = w_idx
        w_idx += n_channel_indices[i] * lengths[i]
        weight_ends[i] = w_idx
        
        channel_starts[i] = c_idx
        c_idx += n_channel_indices[i]
        channel_ends[i] = c_idx
    
    return_points = np.zeros((n_kernels, n_cases, n_channels, n_timepoints), dtype=np.float32)
    for i in prange(n_kernels):
        # Select candidate indices
        if stratified_random_sampling:
            candidate_indices = stratified_random_indices_numba(y, n_cases)
        else:
            candidate_indices = np.random.choice(np.arange(len(X)), size=n_cases, replace=False)
        reshaped_weights = weights[weight_starts[i]:weight_ends[i]].reshape(n_channel_indices[i], lengths[i])
        my_length = lengths[i]
        my_biases = biases[i]
        my_dilations = dilations[i]
        my_paddings = paddings[i]
        my_n_channel = n_channel_indices[i]
        my_channel_indices = channel_indices[channel_starts[i]:channel_ends[i]]
        for j in range(n_cases):
            x = X[candidate_indices[j]]
            if n_channel_indices[i] == 1:
                # Univariate
                univariate_series = x[0]
                univariate_weights = reshaped_weights
                convolved  = np.zeros((1, n_timepoints), dtype=np.float32)
                convolved = univariate_apply_kernel_return_convolved(
                    univariate_series,
                    univariate_weights,
                    my_length,
                    my_biases,
                    my_dilations,
                    my_paddings,
                    convolved
                )
                return_points[i, j] = convolved
            else:
                # Multivariate
                convolved = np.zeros((n_channels, n_timepoints), dtype=np.float32)
                convolved = multivariate_apply_kernel_return_convolved(
                    x,
                    reshaped_weights,
                    my_length,
                    my_biases,
                    my_dilations,
                    my_paddings,
                    my_n_channel,
                    my_channel_indices,
                    convolved
                )
                return_points[i, j] = convolved
    return return_points
    
    
@njit(fastmath=True, cache=True)
# Compute start and end indices for weights and channels
def _calculate_kernel_indices(lengths, n_channel_indices, n_kernels):
    weight_starts = np.zeros(n_kernels, dtype=np.int32)
    weight_ends = np.zeros(n_kernels, dtype=np.int32)
    channel_starts = np.zeros(n_kernels, dtype=np.int32)
    channel_ends = np.zeros(n_kernels, dtype=np.int32)
    
    w_idx = 0
    c_idx = 0

    for i in range(n_kernels): # No prange needed here
        weight_starts[i] = w_idx
        w_idx += n_channel_indices[i] * lengths[i]
        weight_ends[i] = w_idx
        
        channel_starts[i] = c_idx
        c_idx += n_channel_indices[i]
        channel_ends[i] = c_idx
        
    return weight_starts, weight_ends, channel_starts, channel_ends    

#Faster without Numba on small scale data
'''@njit(
    parallel=True,
    fastmath=True,
    cache=True,
)'''
def apply_kernels_proto(X, y, kernels, protos, dist_id, window_frac=.2):
    """
    Calculate distance between selected prototypes and convolved points.
    """
    weights, lengths, biases, dilations, paddings, n_channel_indices, channel_indices = kernels
    n_protos_per_kernel = protos.shape[1]
    
    n_cases, n_channels, n_timepoints = X.shape
    n_kernels = lengths.shape[0]
    
    weight_starts, weight_ends, channel_starts, channel_ends = _calculate_kernel_indices(lengths, n_channel_indices, n_kernels)
    
    return_features = np.zeros((n_cases, n_kernels*n_protos_per_kernel), np.float32)
    for i in range(n_kernels):
        reshaped_weights = weights[weight_starts[i]:weight_ends[i]].reshape(n_channel_indices[i], lengths[i])
        my_length = lengths[i]
        my_biases = biases[i]
        my_dilations = dilations[i]
        my_paddings = paddings[i]
        my_n_channel = n_channel_indices[i]
        my_channel_indices = channel_indices[channel_starts[i]:channel_ends[i]]
        my_protos = protos[i]
        feature_start_col = i * n_protos_per_kernel
        for j in range(n_cases):
            x = X[j]
            if n_channel_indices[i] == 1:
                univariate_series = x[0]
                
                new_features = apply_proto_kernel_univariate_dispatcher(univariate_series, reshaped_weights, my_length, my_biases, my_dilations, my_paddings, my_protos, dist_id, window_frac)
                return_features[j, feature_start_col:feature_start_col + n_protos_per_kernel] = new_features
            else:
                
                new_features = apply_proto_kernel_multivariate_dispatcher(x, reshaped_weights, my_length, my_biases, my_dilations, my_paddings, my_protos, my_n_channel, my_channel_indices, dist_id, window_frac)
                return_features[j, feature_start_col:feature_start_col + n_protos_per_kernel] = new_features
    return return_features
   
    
@njit(fastmath=True, cache=True)
def univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, return_series):
    """Apply a single kernel to a univariate series."""
    n_timepoints = X.shape[0]
    midpoint = length//2
    kernel = weights[0]
    for i in range(0, n_timepoints):
        index = i - midpoint
        convolved_sum = bias
        for j in range(length):
            if index > -1 and index < n_timepoints:
                convolved_sum += kernel[j] * X[index]
            index+=dilation
        return_series[0][i] = convolved_sum
    return return_series
    
@njit(fastmath=True, cache=True)
def multivariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices, return_series):
    """Apply a single kernel to a multivariate series."""
    n_channels, n_timepoints = X.shape
    midpoint = length//2
    kernel = weights[0]
    for kernel_index in range(num_channel_indices):
        kernel = weights[kernel_index]
        X_channel = X[channel_indices[kernel_index]]
        for i in range(0, n_timepoints):
            start_index = i - midpoint
            convolved_sum = bias
            current_index = start_index
            for j in range(length):
                if current_index > -1 and current_index < n_timepoints:
                    convolved_sum += kernel[j] * X_channel[current_index]
                current_index+=dilation
            return_series[kernel_index][i] = convolved_sum
    return return_series
    
    
@njit(inline='always')
def call_distance(dist_id, x, y, window):
    if dist_id == MSM:
        return msm_distance(x, y, window)
    elif dist_id == EUCLIDEAN:
        return euclidean_distance(x, y)
    elif dist_id == MANHATTAN:
        return manhattan_distance(x, y)
    elif dist_id == EMD_UNSORT:
        return compute_signed_emd(x, y)
    elif dist_id == EMD_SORT:
        return sorted_emd_wrapper(x, y)
    elif dist_id == COSINE:
        return cosine_similarity_wrapper(x, y)
    elif dist_id == ADTW:
        return adtw_distance(x, y, window)
    elif dist_id == DTW:
        return dtw_distance(x, y, window)
    elif dist_id == WDTW:
        return wdtw_distance(x, y, window)
    elif dist_id == ERP:
        return erp_distance(x, y, window)
    elif dist_id == LCSS:
        return lcss_distance(x, y, window)
    elif dist_id == TWE:
        return twe_distance(x, y, window)
    elif dist_id == CHEBYSHEV:
        return chebyshev_wrapper(x, y, window)
    elif dist_id == CORRELATION:
        return correlation_wrapper(x, y, window) 

#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_serial_dynamic(X, weights, length, bias, dilation, padding, protos, dist_id, window_frac=.2):
    """Measure a single set of distances for univariate time series protos under convolution."""
    convolved  = np.zeros((1, n_timepoints), dtype=np.float32)
    convolved_x = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding)
    return_array = np.zeros(protos.shape[0], dtype=np.float32)
    for i in range(protos.shape[0]):
        call_distance(dist_id, convolved_x, protos[i], window_frac)
    return return_array
    
    
#Dedicated Functions for Static Distances in parallel and serial.

#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_univariate_euclidean_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    #print(protos.shape)
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = euclidean_distance(convolved, protos[i])
    return return_array
    
@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_euclidean_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = euclidean_distance(convolved, protos[i])
    return return_array
    
@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_manhattan_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = manhattan_distance(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_manhattan_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = manhattan_distance(convolved, protos[i])
    return return_array
   
@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_emdunsort_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = compute_signed_emd(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_emdunsort_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = compute_signed_emd(convolved, protos[i])
    return return_array
    
@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_emdsort_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = sorted_emd_wrapper(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_emdsort_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = sorted_emd_wrapper(convolved, protos[i])
    return return_array
        
@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_cosine_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = cosine_similarity_wrapper(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_cosine_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = cosine_similarity_wrapper(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_chebyshev_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = chebyshev_wrapper(convolved, protos[i])

    return return_array
    
#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_chebyshev_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = chebyshev_wrapper(convolved, protos[i])
    return return_array
       
@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_correlation_parallel(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = correlation_wrapper(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_univariate_correlation_serial(X, weights, length, bias, dilation, padding, protos):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, _, proto_len = protos.shape
    convolved  = np.zeros((1, proto_len), dtype=np.float32) 
    convolved = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = correlation_wrapper(convolved, protos[i])
    return return_array
    
UNIVARIATE_STATIC_PARALLEL_DISPATCH = {
    EUCLIDEAN: _apply_proto_kernel_univariate_euclidean_parallel,
    MANHATTAN: _apply_proto_kernel_univariate_manhattan_parallel,
    EMD_SORT: _apply_proto_kernel_univariate_emdsort_parallel,
    EMD_UNSORT: _apply_proto_kernel_univariate_emdunsort_parallel,
    COSINE: _apply_proto_kernel_univariate_cosine_parallel,
    CORRELATION: _apply_proto_kernel_univariate_correlation_parallel,
    CHEBYSHEV: _apply_proto_kernel_univariate_chebyshev_parallel,
    # ... add new distances here ...
}

UNIVARIATE_STATIC_SERIAL_DISPATCH = {
    EUCLIDEAN: _apply_proto_kernel_univariate_euclidean_serial,
    MANHATTAN: _apply_proto_kernel_univariate_manhattan_serial,
    EMD_SORT: _apply_proto_kernel_univariate_emdsort_serial,
    EMD_UNSORT: _apply_proto_kernel_univariate_emdunsort_serial,
    COSINE: _apply_proto_kernel_univariate_cosine_serial,
    CORRELATION: _apply_proto_kernel_univariate_correlation_serial,
    CHEBYSHEV: _apply_proto_kernel_univariate_chebyshev_serial,
    # add new distances here
}

# Decide what level of parallelization to use.
#@njit(fastmath=True, cache=True)
def apply_proto_kernel_univariate_dispatcher(X, weights, length, bias, dilation, padding, protos, dist_id, window_frac=.2):
    """Dynamically switches between serial and parallel Numba kernels."""
    
    N_PROTOS = protos.shape[0]
    PARALLEL_THRESHOLD = 7 #Cutoff is arbitrary, should be tested properly
    if dist_id in UNIVARIATE_STATIC_PARALLEL_DISPATCH and N_PROTOS >= PARALLEL_THRESHOLD:
        
        func = UNIVARIATE_STATIC_PARALLEL_DISPATCH[dist_id]
        
        return func(X, weights, length, bias, dilation, padding, protos)
    # Unlikely to gain from parallelization
    elif dist_id in UNIVARIATE_STATIC_SERIAL_DISPATCH:
        
        func = UNIVARIATE_STATIC_SERIAL_DISPATCH[dist_id]
        
        return func(X, weights, length, bias, dilation, padding, protos)

    # 3. Fall back to the original slow, dynamic call (for unoptimized distances)
    else:
        return _apply_proto_kernel_univariate_serial_dynamic(X, *kernels, protos, dist_id, window_frac)
        
#Dedicated functions for static, multivariate distances.
        
#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_euclidean_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = euclidean_distance(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_euclidean_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = euclidean_distance(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_manhattan_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = manhattan_distance(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_manhattan_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = manhattan_distance(convolved, protos[i])
    return return_array
    
#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_emdunsort_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = compute_signed_emd(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_emdunsort_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = compute_signed_emd(convolved, protos[i])
    return return_array

#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_emdsort_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = sorted_emd_wrapper(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_emdsort_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = sorted_emd_wrapper(convolved, protos[i])
    return return_array    
    
#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_cosine_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = cosine_similarity_wrapper(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_cosine_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = cosine_similarity_wrapper(convolved, protos[i])
    return return_array    
    
#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_chebyshev_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = chebyshev_wrapper(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_chebyshev_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = chebyshev_wrapper(convolved, protos[i])
    return return_array    

#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_correlation_serial(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in range(proto_count):
        return_array[i] = correlation_wrapper(convolved, protos[i])
    return return_array

@njit(fastmath=True, parallel=True, cache=True)
def _apply_proto_kernel_multivariate_correlation_parallel(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices):
    """Measure a single set of distances for univariate time series protos under convolution."""
    proto_count, proto_channels, proto_len = protos.shape
    convolved  = np.zeros((proto_channels, proto_len), dtype=np.float32) 
    convolved = multivariate_apply_kernel_return_convolved(x, weights, length, biases, dilation, padding, n_channel, channel_indices, convolved)
    return_array = np.zeros(proto_count, dtype=np.float32)
    for i in prange(proto_count):
        return_array[i] = correlation_wrapper(convolved, protos[i])
    return return_array    
    
#@njit(fastmath=True, cache=True)
def _apply_proto_kernel_multivariate_serial_dynamic(X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices, protos, dist_id, window_frac=.2):
    """Measure a single set of distances for multivariate time series protos under convolution."""
    convolved_x = multivariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices)
    return_array = np.zeros(protos.shape[0], dtype=np.float32)
    for i in range(0, protos.shape[0]):
        return_array[i] = call_distance(dist_id, convolved_x, protos[i], window_frac)
    return return_array
    

MULTIVARIATE_STATIC_PARALLEL_DISPATCH = {
    EUCLIDEAN: _apply_proto_kernel_multivariate_euclidean_parallel,
    MANHATTAN: _apply_proto_kernel_multivariate_manhattan_parallel,
    EMD_SORT: _apply_proto_kernel_multivariate_emdsort_parallel,
    EMD_UNSORT: _apply_proto_kernel_multivariate_emdunsort_parallel,
    COSINE: _apply_proto_kernel_multivariate_cosine_parallel,
    CORRELATION: _apply_proto_kernel_multivariate_correlation_parallel,
    CHEBYSHEV: _apply_proto_kernel_multivariate_chebyshev_parallel,
    # ... add new distances here ...
}

MULTIVARIATE_STATIC_SERIAL_DISPATCH = {
    EUCLIDEAN: _apply_proto_kernel_multivariate_euclidean_serial,
    MANHATTAN: _apply_proto_kernel_multivariate_manhattan_serial,
    EMD_SORT: _apply_proto_kernel_multivariate_emdsort_serial,
    EMD_UNSORT: _apply_proto_kernel_multivariate_emdunsort_serial,
    COSINE: _apply_proto_kernel_multivariate_cosine_serial,
    CORRELATION: _apply_proto_kernel_multivariate_correlation_serial,
    CHEBYSHEV: _apply_proto_kernel_multivariate_chebyshev_serial,
    # ... add new distances here ...
}

# Decide what level of parallelization to use.
#@njit(fastmath=True, cache=True)
def apply_proto_kernel_multivariate_dispatcher(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices, dist_id, window_frac):
    """Dynamically switches between serial and parallel Numba kernels."""
    
    N_PROTOS = protos.shape[0]
    PARALLEL_THRESHOLD = 7 #Threshold is arbitrary and should be tested
    if dist_id in MULTIVARIATE_STATIC_PARALLEL_DISPATCH and N_PROTOS >= PARALLEL_THRESHOLD:
        
        func = MULTIVARIATE_STATIC_PARALLEL_DISPATCH[dist_id]
        
        return func(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices)
    # Unlikely to gain from parallelization
    elif dist_id in MULTIVARIATE_STATIC_SERIAL_DISPATCH:
        
        func = MULTIVARIATE_STATIC_SERIAL_DISPATCH[dist_id]
        
        return func(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices)

    # 3. Fall back to the original slow, dynamic call (for unoptimized distances)
    else:
        return _apply_proto_kernel_multivariate_serial_dynamic(x, weights, length, biases, dilation, padding, protos, n_channel, channel_indices, dist_id, window_frac)
